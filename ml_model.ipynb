{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6fb12e-9763-4f6f-8cb2-3116d81a201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs:  5\n",
      "Batch size:  128\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1557162, 115, 1)\n",
      "testX.shape:  (173018, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1557162, 11)\n",
      "testy.shape:  (173018, 11) \n",
      "\n",
      "Data shapes: (1557162, 115, 1) (1557162, 11) (173018, 115, 1) (173018, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Training model with learning rate: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nalla\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m12166/12166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1526s\u001b[0m 125ms/step - accuracy: 0.9093 - loss: 0.1779\n",
      "Epoch 2/5\n",
      "\u001b[1m    1/12166\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35:24\u001b[0m 175ms/step - accuracy: 0.9141 - loss: 0.1761"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nalla\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:156: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12166/12166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1409s\u001b[0m 116ms/step - accuracy: 0.9374 - loss: 0.1040\n",
      "Epoch 3/5\n",
      "\u001b[1m12166/12166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1532s\u001b[0m 126ms/step - accuracy: 0.9391 - loss: 0.1010\n",
      "Epoch 4/5\n",
      "\u001b[1m12166/12166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1617s\u001b[0m 133ms/step - accuracy: 0.9400 - loss: 0.0985\n",
      "Epoch 5/5\n",
      "\u001b[1m12166/12166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1713s\u001b[0m 141ms/step - accuracy: 0.9407 - loss: 0.0970\n",
      "\u001b[1m1352/1352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 55ms/step - accuracy: 0.9415 - loss: 0.0949\n",
      "\u001b[1m1352/1352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 50ms/step\n",
      "Accuracy : 0.9414627379810193\n",
      "F1 Score: 0.9232200724575946\n",
      "Precision: 0.9657742436726245\n",
      "Recall: 0.9414627379810193\n",
      "Confusion Matrix: \n",
      "[[98528     0     0     0     0     1     0     0     0     0     0]\n",
      " [    0  5639     3     1     0     0     0     0     0     0     0]\n",
      " [    0     3  2898     0     0     0     0     0     0     0     0]\n",
      " [    1     0     0  2817     0     0     0     0     0     0     1]\n",
      " [    1     0     0     0    12  8941     0     0     0     0     0]\n",
      " [    1     0     0     0     0 10407     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0  4966     0     0  1143    20]\n",
      " [    0     0     0     0     0     0     0  9633     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0  6676     0     0]\n",
      " [    0     0     0     0     0     0    11     0     0 15737     0]\n",
      " [    0     0     0     0     0     0     0     0     0     1  5577]]\n",
      "\n",
      "Training time: 7799.0337562561035 seconds\n",
      "Testing time: 75.24147748947144 seconds\n",
      "Model output length: 4\n",
      "Duration of loading data: 47 seconds\n",
      "Duration of evaluating model: 7942 seconds\n",
      "Duration of whole experiment: 7989 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.utils import resample, class_weight\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "\n",
    "# define the parameter values\n",
    "epochs, batch_size = 5, 128  # Reduced number of epochs and increased batch size\n",
    "print('Number of epochs: ', epochs)\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.10, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy, num_classes=num_classes)\n",
    "    testy = to_categorical(testy, num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import time\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    n_features, n_added_dimension, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "    # Define your learning rates\n",
    "    learning_rates = [0.0005]\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f'Training model with learning rate: {lr}')\n",
    "\n",
    "        # Define the model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(100, input_shape=(n_features, n_added_dimension), return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv1D(64, 3, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "\n",
    "        # Use early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "        # Measure training time\n",
    "        start_time = time.time()\n",
    "        model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Measure testing time\n",
    "        start_time = time.time()\n",
    "        _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "        testing_time = time.time() - start_time\n",
    "        \n",
    "        # Predict the test set results\n",
    "        y_pred = model.predict(testX, batch_size=batch_size)\n",
    "       \n",
    "\n",
    "        # Convert testy from one-hot encoded to single-digit class labels\n",
    "        testy_single_digit = np.argmax(testy, axis=1)\n",
    "        y_pred_single_digit = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "        # Now calculate the metrics\n",
    "        accuracy = accuracy_score(testy_single_digit, y_pred_single_digit)\n",
    "        f1 = f1_score(testy_single_digit, y_pred_single_digit, average='weighted')\n",
    "        recall = recall_score(testy_single_digit, y_pred_single_digit, average='weighted')\n",
    "        precision = precision_score(testy_single_digit, y_pred_single_digit, average='weighted')\n",
    "        cm = confusion_matrix(testy_single_digit, y_pred_single_digit)\n",
    "\n",
    "        print(f'Accuracy : {accuracy}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'Confusion Matrix: \\n{cm}\\n') \n",
    "        print(f'Training time: {training_time} seconds')\n",
    "        print(f'Testing time: {testing_time} seconds')\n",
    "\n",
    "    return accuracy, f1, recall, cm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "\n",
    "    \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        \n",
    "        \n",
    "     \n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae1cb34-3d67-4342-a3bd-3d4f89502e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1557162, 115, 1)\n",
      "testX.shape:  (173018, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1557162, 11)\n",
      "testy.shape:  (173018, 11) \n",
      "\n",
      "Data shapes: (1557162, 115, 1) (1557162, 11) (173018, 115, 1) (173018, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Accuracy : 0.9675004912783641\n",
      "F1 Score: 0.9679186233750645\n",
      "Recall: 0.9675004912783641\n",
      "Precision: 0.9687541300694644\n",
      "Confusion Matrix: \n",
      "[[98529     0     0     0     0     0     0     0     0     0     0]\n",
      " [   20  5623     0     0     0     0     0     0     0     0     0]\n",
      " [    4     3  2894     0     0     0     0     0     0     0     0]\n",
      " [   48     4     1  2759     0     0     1     5     1     0     0]\n",
      " [   10     3     0     1  8940     0     0     0     0     0     0]\n",
      " [    6     3     0     1     0 10398     0     0     0     0     0]\n",
      " [   36     1     0     2     0     0  4601     0  1179   251    59]\n",
      " [   59     0     0    10     0     0     3  9557     0     2     2]\n",
      " [   41     4     0     0     0     0  1183     1  4833   558    56]\n",
      " [  224     0     0    10     1     0   692     3   451 14361     6]\n",
      " [   35     0     0     3     0     0   448     2   178    12  4900]]\n",
      "\n",
      "Model output length: 5\n",
      "Score: 96.75 \n",
      "\n",
      "Results for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data:\n",
      "Duration of loading data: 26 seconds\n",
      "Duration of evaluating model: 862 seconds\n",
      "Duration of whole experiment: 889 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.10, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy.values.ravel(), num_classes=num_classes)\n",
    "    testy = to_categorical(testy.values.ravel(), num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # Flatten the data for KNN\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "    # Convert categorical labels back to 1-D for KNN\n",
    "    trainy_flat = np.argmax(trainy, axis=1)\n",
    "    testy_flat = np.argmax(testy, axis=1)\n",
    "\n",
    "    # Define the model with n_neighbors=3\n",
    "    model = KNeighborsClassifier(n_neighbors=3)\n",
    "    model.fit(trainX_flat, trainy_flat)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(testX_flat)\n",
    "\n",
    "    # Calculate Accuracy, F1 score, Recall, and Precision\n",
    "    accuracy = accuracy_score(testy_flat, y_pred)\n",
    "    f1 = f1_score(testy_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(testy_flat, y_pred, average='weighted')\n",
    "    precision = precision_score(testy_flat, y_pred, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(testy_flat, y_pred)\n",
    "\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Confusion Matrix: \\n{cm}\\n')       \n",
    "\n",
    "    return accuracy, f1, recall, precision, cm\n",
    "\n",
    "  \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        accuracy= score[0] * 100\n",
    "        print('Score: %.2f' % accuracy, '\\n')\n",
    "        scores.append(score)\n",
    "    print('Results for ' + datasetname + ':')\n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe9b5e7-8da0-43c8-9d16-3e22b45941ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1557162, 115, 1)\n",
      "testX.shape:  (173018, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1557162, 11)\n",
      "testy.shape:  (173018, 11) \n",
      "\n",
      "Data shapes: (1557162, 115, 1) (1557162, 11) (173018, 115, 1) (173018, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Accuracy : 0.7705556647285253\n",
      "F1 Score: 0.7366194988453987\n",
      "Recall: 0.7705556647285253\n",
      "Precision: 0.7532716839724021\n",
      "Confusion Matrix: \n",
      "[[92027     0     0     0     0  6453     0     0     0     1    48]\n",
      " [    0     0     0     0     0     0     0  5643     0     0     0]\n",
      " [    0     0     0     0     0     2     0  2899     0     0     0]\n",
      " [    0     0     0     0     0     2     0  2817     0     0     0]\n",
      " [    0     0     0     0     0  8953     0     0     0     1     0]\n",
      " [    0     0     0     0     0 10408     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0  2726     0     0  3402     1]\n",
      " [    0     0     0     0     0     0     0  9633     0     0     0]\n",
      " [    0     0     0     0     0     2     0  6674     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0 15748     0]\n",
      " [    0     0     0     0     0     1     0  2799     0     0  2778]]\n",
      "\n",
      "Model output length: 5\n",
      "Score: 77.06 \n",
      "\n",
      "Results for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data:\n",
      "Duration of loading data: 20 seconds\n",
      "Duration of evaluating model: 1960 seconds\n",
      "Duration of whole experiment: 1980 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nalla\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.10, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy.values.ravel(), num_classes=num_classes)\n",
    "    testy = to_categorical(testy.values.ravel(), num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # Flatten the data for AdaBoost\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "    # Convert categorical labels back to 1-D for AdaBoost\n",
    "    trainy_flat = np.argmax(trainy, axis=1)\n",
    "    testy_flat = np.argmax(testy, axis=1)\n",
    "\n",
    "    # Define the model with n_estimators=100\n",
    "    model = AdaBoostClassifier(n_estimators=100)\n",
    "    model.fit(trainX_flat, trainy_flat)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(testX_flat)\n",
    "\n",
    "    # Calculate Accuracy, F1 score, Recall, and Precision\n",
    "    accuracy = accuracy_score(testy_flat, y_pred)\n",
    "    f1 = f1_score(testy_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(testy_flat, y_pred, average='weighted')\n",
    "    precision = precision_score(testy_flat, y_pred, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(testy_flat, y_pred)\n",
    "\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Confusion Matrix: \\n{cm}\\n')       \n",
    "\n",
    "    return accuracy, f1, recall, precision, cm\n",
    "\n",
    "\n",
    "  \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        accuracy= score[0] * 100\n",
    "        print('Score: %.2f' % accuracy, '\\n')\n",
    "        scores.append(score)\n",
    "    print('Results for ' + datasetname + ':')\n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29479d6b-5631-42a8-8d69-dd73eed7adec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1557162, 115, 1)\n",
      "testX.shape:  (173018, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1557162, 11)\n",
      "testy.shape:  (173018, 11) \n",
      "\n",
      "Data shapes: (1557162, 115, 1) (1557162, 11) (173018, 115, 1) (173018, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Accuracy : 0.6189356020760846\n",
      "F1 Score: 0.5089084770322791\n",
      "Recall: 0.6189356020760846\n",
      "Precision: 0.4351077233713771\n",
      "Confusion Matrix: \n",
      "[[98529     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 1158     0     0     0     0     0     0     0     0  4485     0]\n",
      " [   56     0     0     0     0     0     0     0     0  2845     0]\n",
      " [ 1769     0     0     0     0     0     0     0     0  1050     0]\n",
      " [ 8954     0     0     0     0     0     0     0     0     0     0]\n",
      " [10408     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 2146     0     0     0     0     0     0     0     0  3983     0]\n",
      " [   66     0     0     0     0     0     0     0     0  9567     0]\n",
      " [ 2333     0     0     0     0     0     0     0     0  4343     0]\n",
      " [ 7190     0     0     0     0     0     0     0     0  8558     0]\n",
      " [ 2790     0     0     0     0     0     0     0     0  2788     0]]\n",
      "\n",
      "Model output length: 5\n",
      "Score: 61.89 \n",
      "\n",
      "Results for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data:\n",
      "Duration of loading data: 44 seconds\n",
      "Duration of evaluating model: 136 seconds\n",
      "Duration of whole experiment: 180 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nalla\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.10, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy.values.ravel(), num_classes=num_classes)\n",
    "    testy = to_categorical(testy.values.ravel(), num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # Flatten the data for Random Forest\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "    # Convert categorical labels back to 1-D for Random Forest\n",
    "    trainy_flat = np.argmax(trainy, axis=1)\n",
    "    testy_flat = np.argmax(testy, axis=1)\n",
    "\n",
    "    # Define the model with depth 4\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=1)\n",
    "    model.fit(trainX_flat, trainy_flat)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(testX_flat)\n",
    "\n",
    "    # Calculate Accuracy, F1 score, Recall, and Precision\n",
    "    accuracy = accuracy_score(testy_flat, y_pred)\n",
    "    f1 = f1_score(testy_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(testy_flat, y_pred, average='weighted')\n",
    "    precision = precision_score(testy_flat, y_pred, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(testy_flat, y_pred)\n",
    "\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Confusion Matrix: \\n{cm}\\n')       \n",
    "\n",
    "    return accuracy, f1, recall, precision, cm\n",
    "  \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        accuracy= score[0] * 100\n",
    "        print('Score: %.2f' % accuracy, '\\n')\n",
    "        scores.append(score)\n",
    "    print('Results for ' + datasetname + ':')\n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b709c399-a163-4255-ac03-6e840999a2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1557162, 115, 1)\n",
      "testX.shape:  (173018, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1557162, 11)\n",
      "testy.shape:  (173018, 11) \n",
      "\n",
      "Data shapes: (1557162, 115, 1) (1557162, 11) (173018, 115, 1) (173018, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Accuracy : 0.8195274480111896\n",
      "F1 Score: 0.7579015889748837\n",
      "Recall: 0.8195274480111896\n",
      "Precision: 0.736694925783623\n",
      "Confusion Matrix: \n",
      "[[98524     0     0     0     0     5     0     0     0     0     0]\n",
      " [   26  5545     0     0     0     0     0    72     0     0     0]\n",
      " [   25  2690     0     0     0     0     0   184     0     2     0]\n",
      " [ 1974     0     0     0     0     0     0   836     0     9     0]\n",
      " [   14     0     0     0     0  8940     0     0     0     0     0]\n",
      " [   11     0     0     0     0 10397     0     0     0     0     0]\n",
      " [    0     0     0     0     0     1     0    14     0  6114     0]\n",
      " [    0     0     0     0     0     0     0  9633     0     0     0]\n",
      " [  266   281     0     0     0     0     0  4181  1948     0     0]\n",
      " [    2     0     0     0     0     0     0     0     0 15746     0]\n",
      " [    1     0     0     0     0     0     0     0     0  5577     0]]\n",
      "\n",
      "Model output length: 5\n",
      "Score: 81.95 \n",
      "\n",
      "Results for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data:\n",
      "Duration of loading data: 25 seconds\n",
      "Duration of evaluating model: 238 seconds\n",
      "Duration of whole experiment: 264 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nalla\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.10, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy.values.ravel(), num_classes=num_classes)\n",
    "    testy = to_categorical(testy.values.ravel(), num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # Flatten the data for Random Forest\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "    # Convert categorical labels back to 1-D for Random Forest\n",
    "    trainy_flat = np.argmax(trainy, axis=1)\n",
    "    testy_flat = np.argmax(testy, axis=1)\n",
    "\n",
    "    # Define the model with depth 4\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=2)\n",
    "    model.fit(trainX_flat, trainy_flat)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(testX_flat)\n",
    "\n",
    "    # Calculate Accuracy, F1 score, Recall, and Precision\n",
    "    accuracy = accuracy_score(testy_flat, y_pred)\n",
    "    f1 = f1_score(testy_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(testy_flat, y_pred, average='weighted')\n",
    "    precision = precision_score(testy_flat, y_pred, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(testy_flat, y_pred)\n",
    "\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Confusion Matrix: \\n{cm}\\n')       \n",
    "\n",
    "    return accuracy, f1, recall, precision, cm\n",
    "  \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        accuracy= score[0] * 100\n",
    "        print('Score: %.2f' % accuracy, '\\n')\n",
    "        scores.append(score)\n",
    "    print('Results for ' + datasetname + ':')\n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43fc3d5-17ee-466b-a251-7c3294aae5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1557162, 115, 1)\n",
      "testX.shape:  (173018, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1557162, 11)\n",
      "testy.shape:  (173018, 11) \n",
      "\n",
      "Data shapes: (1557162, 115, 1) (1557162, 11) (173018, 115, 1) (173018, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Accuracy : 0.9607208498537725\n",
      "F1 Score: 0.9518204415336554\n",
      "Recall: 0.9607208498537725\n",
      "Precision: 0.952289583201261\n",
      "Confusion Matrix: \n",
      "[[98524     0     0     0     0     5     0     0     0     0     0]\n",
      " [    0  5629     0     0     0     0     0     0    14     0     0]\n",
      " [    2  2894     0     0     0     0     0     0     5     0     0]\n",
      " [  134     0     0  2679     0     0     0     0     6     0     0]\n",
      " [   14     0     0     0  8939     1     0     0     0     0     0]\n",
      " [   11     0     0     0     0 10397     0     0     0     0     0]\n",
      " [    0     0     0     0     1     0  3114   161     0  2514   339]\n",
      " [    0     0     0     0     0     0     0  9633     0     0     0]\n",
      " [    2    78     0     0     0     0     0   612  5984     0     0]\n",
      " [    2     0     0     0     0     0     0     0     0 15746     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0  5577]]\n",
      "\n",
      "Model output length: 5\n",
      "Score: 96.07 \n",
      "\n",
      "Results for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data:\n",
      "Duration of loading data: 22 seconds\n",
      "Duration of evaluating model: 321 seconds\n",
      "Duration of whole experiment: 344 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nalla\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.10, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy.values.ravel(), num_classes=num_classes)\n",
    "    testy = to_categorical(testy.values.ravel(), num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # Flatten the data for Random Forest\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "    # Convert categorical labels back to 1-D for Random Forest\n",
    "    trainy_flat = np.argmax(trainy, axis=1)\n",
    "    testy_flat = np.argmax(testy, axis=1)\n",
    "\n",
    "    # Define the model with depth 4\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "    model.fit(trainX_flat, trainy_flat)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(testX_flat)\n",
    "\n",
    "    # Calculate Accuracy, F1 score, Recall, and Precision\n",
    "    accuracy = accuracy_score(testy_flat, y_pred)\n",
    "    f1 = f1_score(testy_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(testy_flat, y_pred, average='weighted')\n",
    "    precision = precision_score(testy_flat, y_pred, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(testy_flat, y_pred)\n",
    "\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Confusion Matrix: \\n{cm}\\n')       \n",
    "\n",
    "    return accuracy, f1, recall, precision, cm\n",
    "  \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        accuracy= score[0] * 100\n",
    "        print('Score: %.2f' % accuracy, '\\n')\n",
    "        scores.append(score)\n",
    "    print('Results for ' + datasetname + ':')\n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8650fc7b-e8b2-4510-af16-f695dbd9c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1557162, 115, 1)\n",
      "testX.shape:  (173018, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1557162, 11)\n",
      "testy.shape:  (173018, 11) \n",
      "\n",
      "Data shapes: (1557162, 115, 1) (1557162, 11) (173018, 115, 1) (173018, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Accuracy : 0.9834121305297715\n",
      "F1 Score: 0.9772692298000288\n",
      "Recall: 0.9834121305297715\n",
      "Precision: 0.9888647288060544\n",
      "Confusion Matrix: \n",
      "[[98528     0     0     0     0     1     0     0     0     0     0]\n",
      " [    0  5642     0     1     0     0     0     0     0     0     0]\n",
      " [    2  2827    72     0     0     0     0     0     0     0     0]\n",
      " [    2     0     0  2817     0     0     0     0     0     0     0]\n",
      " [    3     0     0     0  8950     1     0     0     0     0     0]\n",
      " [   10     0     0     0     0 10398     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0  6107     0     0     3    19]\n",
      " [    0     0     0     0     0     0     0  9633     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0  6676     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0 15748     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0  5577]]\n",
      "\n",
      "Model output length: 5\n",
      "Score: 98.34 \n",
      "\n",
      "Results for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data:\n",
      "Duration of loading data: 19 seconds\n",
      "Duration of evaluating model: 472 seconds\n",
      "Duration of whole experiment: 491 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.10, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy.values.ravel(), num_classes=num_classes)\n",
    "    testy = to_categorical(testy.values.ravel(), num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # Flatten the data for Random Forest\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "    # Convert categorical labels back to 1-D for Random Forest\n",
    "    trainy_flat = np.argmax(trainy, axis=1)\n",
    "    testy_flat = np.argmax(testy, axis=1)\n",
    "\n",
    "    # Define the model with depth 4\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=4)\n",
    "    model.fit(trainX_flat, trainy_flat)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(testX_flat)\n",
    "\n",
    "    # Calculate Accuracy, F1 score, Recall, and Precision\n",
    "    accuracy = accuracy_score(testy_flat, y_pred)\n",
    "    f1 = f1_score(testy_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(testy_flat, y_pred, average='weighted')\n",
    "    precision = precision_score(testy_flat, y_pred, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(testy_flat, y_pred)\n",
    "\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Confusion Matrix: \\n{cm}\\n')       \n",
    "\n",
    "    return accuracy, f1, recall, precision, cm\n",
    "  \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        accuracy= score[0] * 100\n",
    "        print('Score: %.2f' % accuracy, '\\n')\n",
    "        scores.append(score)\n",
    "    print('Results for ' + datasetname + ':')\n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc75098-ffaa-44a6-b923-9aa7de20da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1557162, 115, 1)\n",
      "testX.shape:  (173018, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1557162, 11)\n",
      "testy.shape:  (173018, 11) \n",
      "\n",
      "Data shapes: (1557162, 115, 1) (1557162, 11) (173018, 115, 1) (173018, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Accuracy : 0.9832618571478112\n",
      "F1 Score: 0.9771168114332199\n",
      "Recall: 0.9832618571478112\n",
      "Precision: 0.9886957978297196\n",
      "Confusion Matrix: \n",
      "[[98529     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0  5634     0     9     0     0     0     0     0     0     0]\n",
      " [    2  2824    72     3     0     0     0     0     0     0     0]\n",
      " [    2     0     0  2817     0     0     0     0     0     0     0]\n",
      " [    3     0     0     0  8950     1     0     0     0     0     0]\n",
      " [   10     0     0     0     0 10398     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0  6089     0     0     3    37]\n",
      " [    0     0     0     0     0     0     0  9633     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0  6675     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0 15748     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0  5577]]\n",
      "\n",
      "Model output length: 5\n",
      "Score: 98.33 \n",
      "\n",
      "Results for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data:\n",
      "Duration of loading data: 68 seconds\n",
      "Duration of evaluating model: 947 seconds\n",
      "Duration of whole experiment: 1016 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.10, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy.values.ravel(), num_classes=num_classes)\n",
    "    testy = to_categorical(testy.values.ravel(), num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # Flatten the data for Random Forest\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "    # Convert categorical labels back to 1-D for Random Forest\n",
    "    trainy_flat = np.argmax(trainy, axis=1)\n",
    "    testy_flat = np.argmax(testy, axis=1)\n",
    "\n",
    "    # Define the model with depth 5\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "    model.fit(trainX_flat, trainy_flat)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(testX_flat)\n",
    "\n",
    "    # Calculate Accuracy, F1 score, Recall, and Precision\n",
    "    accuracy = accuracy_score(testy_flat, y_pred)\n",
    "    f1 = f1_score(testy_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(testy_flat, y_pred, average='weighted')\n",
    "    precision = precision_score(testy_flat, y_pred, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(testy_flat, y_pred)\n",
    "\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Confusion Matrix: \\n{cm}\\n')       \n",
    "\n",
    "    return accuracy, f1, recall, precision, cm\n",
    "  \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        accuracy= score[0] * 100\n",
    "        print('Score: %.2f' % accuracy, '\\n')\n",
    "        scores.append(score)\n",
    "    print('Results for ' + datasetname + ':')\n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119e834-da46-4191-907d-407360dd34f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [61.893, 81.952, 96.072, 98.341, 98.336]\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(x, y, marker='o')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Accuracy vs Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4898e741-a158-4d66-98c2-28c7bde1d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "\n",
      "After stacking...\n",
      "trainX.shape:  (1384144, 115, 1)\n",
      "testX.shape:  (346036, 115, 1)\n",
      "\n",
      "After categorizing...\n",
      "trainy.shape:  (1384144, 11)\n",
      "testy.shape:  (346036, 11) \n",
      "\n",
      "Data shapes: (1384144, 115, 1) (1384144, 11) (346036, 115, 1) (346036, 11)\n",
      "Evaluating for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data...\n",
      "\n",
      "Repeat:  1\n",
      "Accuracy : 0.8801483082685039\n",
      "F1 Score: 0.8321224962662769\n",
      "Recall: 0.8801483082685039\n",
      "Precision: 0.8017916817825582\n",
      "Confusion Matrix: \n",
      "[[196835      0      0      0      0      0      0      0     53      0\n",
      "       0]\n",
      " [ 11533      0      0      0      0      0      0      0      0      0\n",
      "       0]\n",
      " [  5764      0      0      0      0      0      0      0      0      0\n",
      "       0]\n",
      " [  5674      0      0      0      0      0      0      0      0      0\n",
      "       0]\n",
      " [    23      0      0      0  17867      4      0      0      0      0\n",
      "       0]\n",
      " [    18      0      0      0      0  20939      0      0      0      0\n",
      "       0]\n",
      " [  7062      0      0      0      0      0   5133      0      0      0\n",
      "       0]\n",
      " [     0      0      0      0      0      0      0  19181      1      0\n",
      "       0]\n",
      " [     0      0      0      0      0      0      1      0  13169      0\n",
      "       0]\n",
      " [    10      0      0      0      0      0      0      0      0  31439\n",
      "       0]\n",
      " [ 11329      0      0      0      0      0      1      0      0      0\n",
      "       0]]\n",
      "\n",
      "Model output length: 5\n",
      "Score: 88.01 \n",
      "\n",
      "Results for C:\\Users\\nalla\\OneDrive\\Provision_PT_838\\dataset\\Data:\n",
      "Duration of loading data: 23 seconds\n",
      "Duration of evaluating model: 34 seconds\n",
      "Duration of whole experiment: 58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nalla\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import mean, std, dstack\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(path):\n",
    "    # Load input data and output data\n",
    "    print('\\nLoading dataset...')\n",
    "    files = os.listdir(path)\n",
    "    dataX_files = [file for file in files if 'x_' in file]\n",
    "    datay_files = [file for file in files if 'y_' in file]\n",
    "    dataX_files.sort()\n",
    "    datay_files.sort()\n",
    "\n",
    "    dataX = pd.concat([pd.read_csv(path + '/' + file, delimiter=',') for file in dataX_files])\n",
    "    \n",
    "    datay = pd.DataFrame()  # Initialize datay as an empty DataFrame\n",
    "    unique_label = 1\n",
    "    for file in datay_files:\n",
    "        datay_temp = pd.read_csv(path + '/' + file, delimiter=',')\n",
    "        if (datay_temp.iloc[:, 0] == 1).any():  # Check if there are any 1 values in the first column\n",
    "            datay_temp.loc[datay_temp.iloc[:, 0] == 1, datay_temp.columns[0]] = unique_label\n",
    "            if datay.empty:  # If datay is empty, directly assign datay_temp\n",
    "                datay = datay_temp\n",
    "            else:\n",
    "                datay = pd.concat([datay, datay_temp])  # Append only the modified datay_temp\n",
    "            unique_label += 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    trainX, testX = model_selection.train_test_split(dataX, test_size=0.20, random_state=42, shuffle=True)\n",
    "    trainy, testy = model_selection.train_test_split(datay, test_size=0.20, random_state=42, shuffle=True)\n",
    "\n",
    "    # Get 3D training data (assuming your data requires this step)\n",
    "    listtrain = [trainX]\n",
    "    listtest = [testX]\n",
    "    trainX = dstack(listtrain)\n",
    "    testX = dstack(listtest)\n",
    "    print('\\nAfter stacking...')\n",
    "    print('trainX.shape: ', trainX.shape)\n",
    "    print('testX.shape: ', testX.shape)\n",
    "\n",
    "    # Calculate number of classes based on unique values in trainy\n",
    "    num_classes = np.unique(trainy).shape[0]\n",
    "\n",
    "    # Convert output data to categorical form\n",
    "    trainy = to_categorical(trainy.values.ravel(), num_classes=num_classes)\n",
    "    testy = to_categorical(testy.values.ravel(), num_classes=num_classes)\n",
    "    print('\\nAfter categorizing...')\n",
    "    print('trainy.shape: ', trainy.shape)\n",
    "    print('testy.shape: ', testy.shape, '\\n')\n",
    "\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix,precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # Flatten the data for Decision Tree\n",
    "    trainX_flat = trainX.reshape(trainX.shape[0], -1)\n",
    "    testX_flat = testX.reshape(testX.shape[0], -1)\n",
    "\n",
    "    # Convert categorical labels back to 1-D for Decision Tree\n",
    "    trainy_flat = np.argmax(trainy, axis=1)\n",
    "    testy_flat = np.argmax(testy, axis=1)\n",
    "\n",
    "    # Define the model\n",
    "    model = DecisionTreeClassifier(max_depth=4)\n",
    "    model.fit(trainX_flat, trainy_flat)\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(testX_flat)\n",
    "\n",
    "    # Calculate Accuracy, F1 score, Recall, and Precision\n",
    "    accuracy = accuracy_score(testy_flat, y_pred)\n",
    "    f1 = f1_score(testy_flat, y_pred, average='weighted')\n",
    "    recall = recall_score(testy_flat, y_pred, average='weighted')\n",
    "    precision = precision_score(testy_flat, y_pred, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(testy_flat, y_pred)\n",
    "\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Confusion Matrix: \\n{cm}\\n')       \n",
    "\n",
    "    return accuracy, f1, recall, precision, cm\n",
    "  \n",
    "def run_experiment(repeats, datasetname):\n",
    "    experiment_start_time = time.time()\n",
    "    trainX, trainy, testX, testy = load_dataset(datasetname)\n",
    "    print('Data shapes:', trainX.shape, trainy.shape, testX.shape, testy.shape)  # Debugging line\n",
    "    print('Evaluating for ' + datasetname + '...\\n')\n",
    "    evaluation_start_time = time.time()\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print('Repeat: ', r + 1) \n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        print('Model output length:', len(score))  # Corrected line\n",
    "        accuracy= score[0] * 100\n",
    "        print('Score: %.2f' % accuracy, '\\n')\n",
    "        scores.append(score)\n",
    "    print('Results for ' + datasetname + ':')\n",
    "    \n",
    "    print(\"Duration of loading data: %d seconds\" % (evaluation_start_time - experiment_start_time))\n",
    "    print(\"Duration of evaluating model: %d seconds\" % (time.time() - evaluation_start_time))\n",
    "    print(\"Duration of whole experiment: %d seconds\" % (time.time() - experiment_start_time))\n",
    "\n",
    "run_experiment(1, 'C:\\\\Users\\\\nalla\\\\OneDrive\\\\Provision_PT_838\\\\dataset\\\\Data')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3576618-b503-4734-9c85-81f8c967e039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
